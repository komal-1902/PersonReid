# -*- coding: utf-8 -*-
"""Explainability_Left_Right_Person_Reid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uXBDM0gEXGOlszXoeFRD8lHfO5LqbJET
"""

! pip install ultralytics

!pip install lime

import numpy as np
import os
import random
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from google.colab import drive
from google.colab.patches import cv2_imshow
import cv2
from ultralytics import YOLO
import matplotlib.pyplot as plt
import lime
from lime import lime_image
from skimage.segmentation import quickshift

drive.mount('/content/drive')
os.chdir('/content/drive/MyDrive/viz_project')

net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
# yolov3.cfg: https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg
# yolov3.weights: https://pjreddie.com/media/files/yolov3.weights
# coco.names: https://github.com/pjreddie/darknet/blob/master/data/coco.names
classes = []
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
layer_names = net.getLayerNames()
output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]

model = YOLO('yolov8n.pt')

model.classes = [0]
model.overrides['conf'] = 0.5
model.overrides['iou'] = 0.45
model.overrides['agnostic_nms'] = False
model.overrides['max_det'] = 1000

# Load pre-trained ResNet50 model
resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')

explainer = lime_image.LimeImageExplainer()

person_colors = {}

def preprocess_image(img):
    img = cv2.resize(img, (224, 224))
    img = preprocess_input(img)
    return img

def extract_person_features(person_roi):
    person_roi = cv2.resize(person_roi, (224, 224))
    person_roi = preprocess_input(person_roi)
    person_roi = np.expand_dims(person_roi, axis=0)
    features = resnet_model.predict(person_roi)
    return features

def extract_features(frame, boxes):
    features = []
    updated_boxes = []
    for (x1, y1, x2, y2) in boxes:
        person_roi = frame[y1:y2, x1:x2]
        # print(person_roi)
        if person_roi.size == 0:
            continue
        feature_vector = extract_person_features(person_roi)
        features.append(feature_vector.flatten())
        updated_boxes.append((x1, y1, x2, y2))
    return np.array(features), updated_boxes

def detect_pedestrians_yolo_v3(frame):
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    boxes = []
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5 and class_id == 0:
                center_x = int(detection[0] * frame.shape[1])
                center_y = int(detection[1] * frame.shape[0])
                w = int(detection[2] * frame.shape[1])
                h = int(detection[3] * frame.shape[0])
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, x + w, y + h])
    return boxes

def detect_pedestrians_yolo_v8(frame):
    results = model.predict(frame, cls=0)
    boxes = []
    for det in results[0].boxes:
        xmin, ymin, xmax, ymax = det.xyxy[0].cpu().numpy()
        conf = det.conf
        cls = det.cls.cpu().numpy()[0]
        if cls == 0:
            boxes.append((xmin, ymin, xmax, ymax))
    new_boxes = [(int(x1), int(y1), int(x2), int(y2)) for (x1, y1, x2, y2) in boxes]
    return new_boxes

def match_persons(features1, features2):
    matches = []
    for feature1 in features1:
        similarities = [cosine_similarity([feature1], [feature2])[0][0] for feature2 in features2]
        best_match_idx = np.argmax(similarities)
        best_similarity = similarities[best_match_idx]
        if best_similarity > 0.7:
            matches.append(best_match_idx)
    return matches

def generate_explanations(frame, model, explainer):
    explanation = explainer.explain_instance(frame, model.predict, top_labels=1, hide_color=0, num_samples=1000)
    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
    return temp, maskma

def generate_explanations_for_boxes(frame, model, explainer, boxes):
    explanations = []
    for (x1, y1, x2, y2) in boxes:
        roi = frame[y1:y2, x1:x2]
        preprocessed_roi = preprocess_input(cv2.resize(roi, (224, 224)))
        explanation = explainer.explain_instance(preprocessed_roi, model.predict, top_labels=1, hide_color=0, num_samples=100)
        temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
        mask_resized = cv2.resize(temp, (x2-x1, y2-y1))
        explanations.append(mask_resized)
    return explanations

def draw_matched_bounding_boxes(frame1, frame2, boxes1, boxes2, matches):
    global person_colors
    (x1_1, y1_1, x2_1, y2_1) = (0, 0, 0, 0)
    (x1_2, y1_2, x2_2, y2_2) = (0, 0, 0, 0)
    color = (255, 255, 255)

    for i, match_idx in enumerate(matches):
        (x1_1, y1_1, x2_1, y2_1) = boxes1[i]
        (x1_2, y1_2, x2_2, y2_2) = boxes2[match_idx]
        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
        cv2.rectangle(frame1, (x1_1, y1_1), (x2_1, y2_1), color, 8)
        cv2.rectangle(frame2, (x1_2, y1_2), (x2_2, y2_2), color, 8)

    fig, ax = plt.subplots(1, 2, figsize=(20, 10))
    plt.subplots_adjust(wspace=0.05)

    frame1_copy = frame1.copy()
    cv2.rectangle(frame1_copy, (x1_1, y1_1), (x2_1, y2_1), color, 8)
    ax[0].imshow(cv2.cvtColor(frame1_copy, cv2.COLOR_BGR2RGB)e)
    ax[0].axis('off')

    frame2_copy = frame2.copy()
    cv2.rectangle(frame2_copy, (x1_2, y1_2), (x2_2, y2_2), color, 8)
    ax[1].imshow(cv2.cvtColor(frame2_copy, cv2.COLOR_BGR2RGB))
    ax[1].axis('off')

desired_timestamp = ((12 * 60) + (40)) * 1000

camera1_video = cv2.VideoCapture("chase1_sensor1_left.mp4")
camera1_video.set(cv2.CAP_PROP_POS_MSEC, desired_timestamp)

camera2_video = cv2.VideoCapture("chase1_sensor3_left.mp4")
camera2_video.set(cv2.CAP_PROP_POS_MSEC, desired_timestamp)

frame_count = 0

while True and frame_count < 40:

    ret1, frame1 = camera1_video.read()
    ret2, frame2 = camera2_video.read()

    if not ret1 or not ret2:
        break

    frame_count += 1
    # if frame_count < 10:
    #   continue
    print(frame_count)

    boxes1 = detect_pedestrians_yolo_v8(frame1)
    boxes2 = detect_pedestrians_yolo_v8(frame2)

    explanations1 = generate_explanations_for_boxes(frame1, resnet_model, explainer, boxes1)
    explanations2 = generate_explanations_for_boxes(frame2, resnet_model, explainer, boxes2)

    fig, ax = plt.subplots(1, 2, figsize=(12, 12))

    for i, (x1, y1, x2, y2) in enumerate(boxes1):
        ax[0].imshow(cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB))
        ax[0].add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none'))
        ax[0].imshow(explanations1[i], alpha=0.6, extent=(x1, x2, y2, y1))
        ax[0].axis('off')
        ax[0].set_title('Explanation Frame 1')

    for i, (x1, y1, x2, y2) in enumerate(boxes2):
        ax[1].imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))
        ax[1].add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none'))
        ax[1].imshow(explanations2[i], alpha=0.6, extent=(x1, x2, y2, y1))
        ax[1].axis('off')
        ax[1].set_title('Explanation Frame 2')

    plt.tight_layout()
    plt.show()

    features1, new_boxes1 = extract_features(frame1, boxes1)
    features2, new_boxes2 = extract_features(frame2, boxes2)

    matches = match_persons(features1, features2)

    draw_matched_bounding_boxes(frame1, frame2, new_boxes1, new_boxes2, matches)

    for i, (x1, y1, x2, y2) in enumerate(boxes1):
        explanation = explanations1[i]
        frame1[y1:y1+explanation.shape[0], x1:x1+explanation.shape[1]] = explanation

    for i, (x1, y1, x2, y2) in enumerate(boxes2):
        explanation = explanations2[i]
        frame2[y1:y1+explanation.shape[0], x1:x1+explanation.shape[1]] = explanation

    draw_matched_bounding_boxes(frame1, frame2, new_boxes1, new_boxes2, matches)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

camera1_video.release()
camera2_video.release()
cv2.destroyAllWindows()

